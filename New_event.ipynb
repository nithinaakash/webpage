{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blog.scrapinghub\n",
      "MS_Dhoni\n",
      "forbes.com/sites/jaysondemers\n",
      "uipath\n",
      "background: linear-gradient(rgba(13,23,48,0.4), rgba(13,23,48,0.4)), url(https://www.uipath.com/hubfs/2000x800%20blogpost-%20static%202.jpg) no-repeat center center; background-size:cover; -webkit-background-size: cover; -moz-background-size: cover;  -o-background-size: cover;\n",
      "https://www.uipath.com/hubfs/2000x800%20blogpost-%20static%202.jpg\n",
      "Community members approach the Academy content with a refreshing and exciting attitude. Frankly, I constantly stumble upon LinkedIn posts of people who are celebrating completing their Academy courses or passing the RPA Developer Advanced certification exam. The \"learn anywhere\" ethos reverberates from one continent to another, enabling people everywhere to accelerate their careers.\n",
      "909900090\n",
      "Community members approach the Academy content with a refreshing and exciting attitude. Frankly, I constantly stumble upon LinkedIn posts of people who are celebrating completing their Academy courses or passing the RPA Developer Advanced certification exam. The \"learn anywhere\" ethos reverberates from one continent to another, enabling people everywhere to accelerate their careers.\n",
      "How UiPath is Training People for the Future of Work\n",
      "uipath\n",
      "https://www.uipath.com/blog/sapphire-now-2019-day-2-highlights\n",
      "\n",
      "9090\n",
      "background: linear-gradient(rgba(13,23,48,0.4), rgba(13,23,48,0.4)), url(https://www.uipath.com/hubfs/UiPath_SAP_Partner_Sapphire_Now_2019.jpg) no-repeat center center; background-size:cover; -webkit-background-size: cover; -moz-background-size: cover;  -o-background-size: cover;\n",
      "https://www.uipath.com/hubfs/UiPath_SAP_Partner_Sapphire_Now_2019.jpg\n",
      "Day two for UiPath at the SAPPHIRE NOW 2019 and ASUG Annual Conference is a wrap! As a new SAP Partner, it’s been a delight to “introduce ourselves” at this annual SAP conference. The first day of the conference raised the bar and day two delivered: the enthusiasm about RPA, UiPath as a SAP partner, the great conversations at our booth, and more, has our team energized and excited.\n",
      "Day two for UiPath at the SAPPHIRE NOW 2019 and ASUG Annual Conference is a wrap! As a new SAP Partner, it’s been a delight to “introduce ourselves” at this annual SAP conference. The first day of the conference raised the bar and day two delivered: the enthusiasm about RPA, UiPath as a SAP partner, the great conversations at our booth, and more, has our team energized and excited.\n",
      "background: linear-gradient(rgba(13,23,48,0.4), rgba(13,23,48,0.4)), url(https://www.uipath.com/hubfs/sapphire_now_2019_key_takeaways_rpa_and_sap.jpg) no-repeat center center; background-size:cover; -webkit-background-size: cover; -moz-background-size: cover;  -o-background-size: cover;\n",
      "https://www.uipath.com/hubfs/sapphire_now_2019_key_takeaways_rpa_and_sap.jpg\n",
      "Community members approach the Academy content with a refreshing and exciting attitude. Frankly, I constantly stumble upon LinkedIn posts of people who are celebrating completing their Academy courses or passing the RPA Developer Advanced certification exam. The \"learn anywhere\" ethos reverberates from one continent to another, enabling people everywhere to accelerate their careers.\n",
      "May 23\n",
      "May 29\n",
      "Find out how RKH Specialty, part of the Hyperion Group and one of the fastest growing independent global brokers, uses automation as an enabler to meet strategic goals.\n",
      "Learn how RPA has digitally transformed Virgin Media to increase customer satisfaction and anticipate customer demand.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image,ImageDraw,ImageFont\n",
    "import enchant\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib.request\n",
    "from xml.dom.minidom import parse\n",
    "import xml.dom.minidom\n",
    "import bs4\n",
    "import io\n",
    "import dateutil.parser\n",
    "from apiclient.discovery import build\n",
    "import tweepy\n",
    "import twitter_credential as tc\n",
    "from tweepy import API \n",
    "from tweepy import Cursor\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import time\n",
    "from time import sleep\n",
    "auth =OAuthHandler(tc.consumer_token, tc.consumer_sec)\n",
    "api = tweepy.API(auth)\n",
    "tweedt=[]\n",
    "d = enchant.Dict(\"en_US\")\n",
    "for tweet in tweepy.Cursor(api.search, q='#UiPath').items(3):\n",
    "    a=tweet.text\n",
    "    tweedt.append(a)\n",
    "api_key='AIzaSyAnUa1giBXsa-1dZ6nwXl-WE5i8mFgmZto'\n",
    "url = 'https://www.uipath.com/solutions/webinars'\n",
    "list1=[]\n",
    "list2=[]\n",
    "text1=[]\n",
    "text2=[]\n",
    "date1=[]\n",
    "date2=[]\n",
    "para1=[]\n",
    "para2=[]\n",
    "arg=[]\n",
    "brief=[]\n",
    "yti=[]\n",
    "ydat=[]\n",
    "youtube=build('youtube','v3',developerKey=api_key)\n",
    "def get_channel_videos(name):\n",
    "    res = youtube.channels().list(forUsername=name, \n",
    "                                  part='contentDetails').execute()\n",
    "    playlist_id = res['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    \n",
    "    videos = []\n",
    "    res = youtube.playlistItems().list(playlistId=playlist_id, \n",
    "                                           part='snippet', \n",
    "                                           maxResults=3).execute()\n",
    "    videos += res['items']\n",
    "    \n",
    "    return videos\n",
    "videos = get_channel_videos('UiPath')\n",
    "o=988\n",
    "for i in videos:\n",
    "    yti.append(i['snippet']['title'])\n",
    "    ydat.append(dateutil.parser.parse(i['snippet']['publishedAt']))\n",
    "    img_url=i['snippet']['thumbnails']['default']['url']\n",
    "    imge = Image.open(requests.get(img_url, stream = True).raw)\n",
    "    imge.save(str(o)+'.png')\n",
    "    o=o+1\n",
    "iimg = Image.new('RGB', (1000,1000), color = 'white')\n",
    "iimg.save('white.png')\n",
    "DOMTree = xml.dom.minidom.parse(\"xml_config.xml\")\n",
    "collection = DOMTree.documentElement\n",
    "q=0\n",
    "\n",
    "font = ImageFont.truetype('arial.ttf', size=23)\n",
    "color = 'rgb(95,158,60)'\n",
    "name = collection.getElementsByTagName(\"title\")\n",
    "li=[line for line in open('hello2.csv')]\n",
    "g=0\n",
    "for m in name:\n",
    "    if m.hasAttribute(\"url\"):\n",
    "        type = m.getElementsByTagName('image')[0]\n",
    "        app=m.getElementsByTagName('div')[0]\n",
    "        a=m.getAttribute(\"url\")\n",
    "        print(str(a))\n",
    "        if( str(a) in li[0] ):\n",
    "            alt=type.getAttribute(\"class\")\n",
    "            hyper=app.getAttribute(\"id\")\n",
    "            with urllib.request.urlopen(li[0])as f:\n",
    "                soup=bs4.BeautifulSoup(f)\n",
    "                data = soup.find('div',attrs={'class':alt})\n",
    "                o=data['style']\n",
    "                print(data['style'])\n",
    "                x=o.find(\"https\")\n",
    "                y=o.find(\".jpg\")\n",
    "                img_url=o[x:y+4]\n",
    "                print(img_url)\n",
    "               \n",
    "                imge = Image.open(requests.get(img_url, stream = True).raw)\n",
    "                imge.save('one.png')\n",
    "                \n",
    "                \n",
    "            k=soup.findAll('p')\n",
    "            l=\"s\"\n",
    "            for i in k:\n",
    "                am=i.text\n",
    "                g=g+1\n",
    "                if(g==16):\n",
    "                    l=am\n",
    "            print(l)\n",
    "            print(909900090)\n",
    "            arg.append(l)\n",
    "            print(arg[0])\n",
    "            h=soup.find('h1')\n",
    "            for i in h:\n",
    "                am=i.text\n",
    "                 \n",
    "            print(am)\n",
    "            brief.append(am)\n",
    "        if(str(a) in li[1]):\n",
    "            print(str(a))\n",
    "            print(li[1])\n",
    "            print(9090)\n",
    "            alt=type.getAttribute(\"class\")\n",
    "            hyper=app.getAttribute(\"id\")\n",
    "            with urllib.request.urlopen(li[1])as f:\n",
    "                soup=bs4.BeautifulSoup(f)\n",
    "                data = soup.find('div',attrs={'class':alt})\n",
    "                o=data['style']\n",
    "                print(data['style'])\n",
    "                x=o.find(\"https\")\n",
    "                y=o.find(\".jpg\")\n",
    "\n",
    "                img_url=o[x:y+4]\n",
    "                print(img_url)\n",
    "               \n",
    "                imge = Image.open(requests.get(img_url, stream = True).raw)\n",
    "                imge.save('two.png')\n",
    "            new=soup.find('h1')\n",
    "            am=new.text\n",
    "            brief.append(am) \n",
    "            f=soup.findAll('p')\n",
    "            g=0\n",
    "            for pu in f:               \n",
    "                g=g+1\n",
    "                \n",
    "                if(g==4):\n",
    "                    print(pu.text)\n",
    "                    love=pu.text\n",
    "            print(love)\n",
    "            m=love\n",
    "            arg.append(m)\n",
    "        if(str(a) in li[2]):\n",
    "            alt=type.getAttribute(\"class\")\n",
    "            hyper=app.getAttribute(\"id\")\n",
    "            with urllib.request.urlopen(li[2])as f:\n",
    "                soup=bs4.BeautifulSoup(f)\n",
    "                data = soup.find('div',attrs={'class':alt})\n",
    "                o=data['style']\n",
    "                print(data['style'])\n",
    "                x=o.find(\"https\")\n",
    "                y=o.find(\".jpg\")\n",
    "\n",
    "                img_url=o[x:y+4]\n",
    "                print(img_url)\n",
    "               \n",
    "                imge = Image.open(requests.get(img_url, stream = True).raw)\n",
    "                imge.save('three.png')\n",
    "                h=soup.find('h1')\n",
    "                play=h.text  \n",
    "                brief.append(play)\n",
    "                k = soup.findAll('p')\n",
    "                g=0\n",
    "                for goal in k:\n",
    "                    g=g+1\n",
    "                    if(g==15):\n",
    "                        balel=goal.text\n",
    "                last=balel\n",
    "                arg.append(last)\n",
    "print(arg[0])\n",
    "# print(brief[0])\n",
    "with urllib.request.urlopen(url)as f:\n",
    "    soup=bs4.BeautifulSoup(f)\n",
    "    data=soup.findAll('section',attrs={'class':\"Webinars w-upcoming\"})\n",
    "    for i in data:\n",
    "        try:\n",
    "            l=i.findAll('h4')\n",
    "            text1+=l\n",
    "            go=i.findAll('p')\n",
    "            para1+=go\n",
    "            p=i.findAll('time')\n",
    "            date1 +=p\n",
    "        except KeyError:\n",
    "            pass\n",
    "        f=i.findAll('img')\n",
    "        list1 +=f\n",
    "    for o in list1:\n",
    "        list2.append(o['src'])\n",
    "    for tex in text1:\n",
    "        text2.append(tex.text)\n",
    "    for d in date1:\n",
    "        date2.append(d.text)\n",
    "    for d in date2:\n",
    "        print(d)\n",
    "    for p in para1:\n",
    "        para2.append(p.text)\n",
    "    for p in para2:\n",
    "        print(p)\n",
    "k=0\n",
    "x=0\n",
    "y=0\n",
    "for i in list2:\n",
    "    img_url=i\n",
    "    imge = Image.open(requests.get(img_url, stream = True).raw)\n",
    "    imge.save('one.png')\n",
    "    imge=Image.open('one.png')\n",
    "    width = 250\n",
    "    height = 250\n",
    "    im2 = imge.resize((width, height))\n",
    "    im2.save(str(k)+'.png')\n",
    "    \n",
    "    large=Image.open('white.png')\n",
    "    small=Image.open(str(k)+'.png')\n",
    "    large.paste(small,(x,y))\n",
    "    large.save('white.png')\n",
    "    y=y+300\n",
    "    k=k+1\n",
    "font = ImageFont.truetype('arial.ttf', size=23)\n",
    "color = 'rgb(95,158,60)'\n",
    "print(0)\n",
    "x=20\n",
    "y=260\n",
    "for d in date2:\n",
    "  \n",
    "    image = Image.open('white.png')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "   \n",
    "    draw.text((x,y),d, fill=color, font=font)\n",
    "    y=y+300\n",
    "    image.save('white.png')\n",
    "color = 'rgb(255,0,0)'\n",
    "x=270\n",
    "y=20\n",
    "for t in text2:\n",
    "    image = Image.open('white.png')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    p=0;q=58;\n",
    "    j=y\n",
    "    while(j<y+80):\n",
    "       \n",
    "        name=t[p:q]\n",
    "        draw.text((x,j), name, fill=color, font=font) \n",
    "        p=p+58\n",
    "        q=q+58\n",
    "        j=j+25\n",
    "    y+=300\n",
    "    image.save('white.png')\n",
    "x=270\n",
    "y=100\n",
    "color = 'rgb(0,0,0)'\n",
    "for go in para2:\n",
    "    image = Image.open('white.png')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    p=0;q=58;\n",
    "    j=y\n",
    "    while(j<y+150):\n",
    "        name=go[p:q]\n",
    "        draw.text((x,j), name, fill=color, font=font) \n",
    "        p=p+58\n",
    "        q=q+58\n",
    "        j=j+25\n",
    "    y+=300\n",
    "    image.save('white.png')\n",
    "with open(\"webpagemain.html\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "    soup = BeautifulSoup(contents)\n",
    "    f=soup.find('p',attrs={'id':\"001\"})\n",
    "    new_tag = soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = text2[0]\n",
    "    f=soup.find('td',attrs={'id':\"new\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string=para2[0]\n",
    "    f=soup.find('td',attrs={'id':\"002\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = text2[1]\n",
    "    f=soup.find('td',attrs={'id':\"003\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = para2[1]\n",
    "    f=soup.find('td',attrs={'id':\"1\"})\n",
    "    new_tag = soup.new_tag(\"a\", href=\"https://www.uipath.com/blog/uipath-academy-trains-for-future-of-work\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = brief[0]\n",
    "    f=soup.find('td',attrs={'id':\"2\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = arg[0]\n",
    "    f=soup.find('td',attrs={'id':\"3\"})\n",
    "    new_tag=soup.new_tag(\"a\", href=\"https://www.uipath.com/blog/sapphire-now-2019-day-2-highlights\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = brief[1]\n",
    "    f=soup.find('td',attrs={'id':\"4\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = arg[1]\n",
    "    f=soup.find('td',attrs={'id':\"5\"})\n",
    "    new_tag=soup.new_tag(\"a\", href=\"https://www.uipath.com/blog/sapphire-now-2019-key-takeaways\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string =brief[2]\n",
    "    f=soup.find('td',attrs={'id':\"6\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = arg[2]\n",
    "    f=soup.find('td',attrs={'id':\"y-0\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = yti[0]\n",
    "    f=soup.find('td',attrs={'id':\"y-1\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = yti[1]\n",
    "    f=soup.find('td',attrs={'id':\"y-2\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = yti[2]\n",
    "    f=soup.find('p',attrs={'id':\"love\"})\n",
    "    new_tag=soup.new_tag(\"p\")\n",
    "    f.append(new_tag)\n",
    "    new_tag.string = tweedt[1]\n",
    "    html = soup.prettify(\"utf-8\")\n",
    "with open(\"output.html\", \"wb\") as file:\n",
    "    file.write(html)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
